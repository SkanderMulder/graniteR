---
title: "Technical Approaches: Embeddings vs Fine-Tuning"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Technical Approaches: Embeddings vs Fine-Tuning}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

## Overview

The graniteR package provides two distinct approaches for text classification using IBM's Granite-R2 embedding model (149M parameters). This vignette explains the technical differences, trade-offs, and appropriate use cases for each approach.

## Setup Requirements

This vignette assumes you have graniteR installed with Python dependencies using UV:

```bash
# Run from the graniteR package directory
./setup_python.sh
```

Then configure R:

```r
Sys.setenv(RETICULATE_PYTHON = ".venv/bin/python")
library(graniteR)
```

UV provides fast installation (1-2 minutes vs 10-20 minutes with pip). See the "Getting Started" vignette for complete installation instructions.

## Approach 1: Fixed Embeddings with Separate Classifier

### Methodology

This approach treats the Granite-R2 model as a feature extractor:

1. Generate fixed 768-dimensional embeddings using `textEmbed()`
2. Train a downstream classifier (e.g., random forest, ridge regression) on these embeddings
3. The neural network weights remain frozen

```{r}
library(graniteR)
library(dplyr)

# Generate embeddings (frozen features)
data <- tibble(
  text = c("positive example", "negative example"),
  label = c(1, 0)
)

embeddings <- data |>
  granite_embed(text_col = text)

# Embeddings are now static features for any downstream model
```

### Characteristics

**Computational Efficiency**

- Runs on CPU without performance degradation
- Embedding extraction is fast (single forward pass)
- Downstream training is lightweight
- No gradient computation required

**Data Requirements**

- Effective with small datasets (100-1000 examples)
- Pre-trained embeddings capture general semantic information
- Less prone to overfitting

**Performance**

- Strong baseline performance on many tasks
- Effective when pre-training domain matches application domain
- May underperform on highly specialized or technical text

### When to Use

- Quick prototyping and experimentation
- Limited computational resources (CPU-only environments)
- Small training datasets
- General-purpose classification tasks
- Interpretability is important (downstream model can be simpler)

## Approach 2: End-to-End Fine-Tuning

### Methodology

This approach integrates a classification head directly into the neural network:

1. Add a linear classification layer to Granite-R2 output
2. Train the entire model end-to-end with backpropagation
3. Both embeddings and classifier weights update during training

```{r}
# Create classifier with integrated head
classifier <- granite_classifier(num_labels = 2)

# Fine-tune entire model
classifier <- classifier |>
  granite_train(
    data,
    text_col = text,
    label_col = label,
    epochs = 5,
    learning_rate = 2e-5
  )
```

### Characteristics

**Computational Requirements**

- GPU strongly recommended for practical training times
- Higher memory consumption (stores gradients for all parameters)
- Longer training duration (multiple epochs)

**Data Requirements**

- Benefits from larger datasets (1000+ examples)
- Requires sufficient data to avoid overfitting all 149M parameters
- Validation split essential for monitoring generalization

**Performance**

- Superior accuracy on domain-specific tasks
- Embeddings adapt to capture task-relevant features
- Can learn subtle patterns not captured by fixed embeddings

### When to Use

- Domain-specific applications (medical, legal, technical text)
- Sufficient training data available
- GPU resources accessible
- Maximum accuracy is critical
- Task requires specialized language understanding

## Multi-Task and Multi-Label Classification

### Multi-Label Classification

Single input, multiple binary outputs (labels are not mutually exclusive):

```{r}
# Example: Classify text with multiple properties
# Label 1: Is it formal? Label 2: Is it technical? Label 3: Is it persuasive?

classifier <- granite_classifier(num_labels = 3)

# Training data has multiple binary labels per example
train_data <- tibble(
  text = c("The study demonstrates...", "Buy now!"),
  formal = c(1, 0),
  technical = c(1, 0),
  persuasive = c(0, 1)
)

# Note: Current implementation assumes single label column
# Multi-label requires custom implementation
```

### Multi-Task Learning

Multiple related classification tasks with shared embeddings:

```{r}
# Example: Sentiment (positive/negative) AND topic (sports/politics/tech)
# Two separate heads, shared Granite-R2 backbone

# This requires custom Python implementation:
# - Define model with two classification heads
# - Compute combined loss (loss_sentiment + loss_topic)
# - Backpropagate through shared embeddings
```

Multi-task learning enables knowledge sharing across tasks, improving generalization when tasks are related. The shared embeddings learn representations useful for all tasks simultaneously.

## Technical Implementation Details

### Embedding Extraction

The Granite-R2 model produces token-level embeddings. For sentence classification, these must be aggregated:

**Mean Pooling** (default in graniteR):

```python
def mean_pooling(model_output, attention_mask):
    token_embeddings = model_output[0]
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size())
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    return sum_embeddings / sum_mask
```

**CLS Token** (alternative):

```python
cls_embedding = model_output[0][:, 0, :]
```

### Fine-Tuning Strategy

**Learning Rate Selection**

Transformer fine-tuning typically uses learning rates of 2e-5 to 5e-5. Higher rates risk catastrophic forgetting of pre-trained knowledge.

**Layer Freezing**

For limited data, freeze lower layers and fine-tune only upper layers:

```python
# Freeze first 6 layers (out of 12)
for param in model.base_model.encoder.layer[:6].parameters():
    param.requires_grad = False
```

**Gradient Accumulation**

Simulate larger batch sizes with limited memory:

```r
# Effective batch size = batch_size * accumulation_steps
# Example: batch_size=4, accumulation_steps=4 gives effective batch of 16
```

## Performance Comparison

### Benchmark Results

The following benchmark compares the two approaches using a sample classification task:

```{r benchmark, warning = FALSE, message = FALSE}
set.seed(42)

# Create sample training data
train_data <- tibble(
  text = c(
    "This product exceeded my expectations",
    "Terrible quality, waste of money",
    "Great value for the price",
    "Do not recommend, very disappointed",
    "Amazing features and performance",
    "Poor customer service experience"
  ),
  label = c(1, 0, 1, 0, 1, 0)
)

test_data <- tibble(
  text = c(
    "Excellent purchase, highly satisfied",
    "Not worth the money"
  ),
  label = c(1, 0)
)

# Approach 1: Fixed embeddings
time_embed_start <- Sys.time()
train_embeddings <- train_data |>
  granite_embed(text_col = text)
test_embeddings <- test_data |>
  granite_embed(text_col = text)
time_embed_end <- Sys.time()

# Train simple classifier on embeddings
if (requireNamespace("glmnet", quietly = TRUE)) {
  embed_matrix <- as.matrix(train_embeddings |> select(starts_with("dim_")))
  test_matrix <- as.matrix(test_embeddings |> select(starts_with("dim_")))

  ridge_model <- glmnet::cv.glmnet(
    x = embed_matrix,
    y = train_data$label,
    alpha = 0,
    family = "binomial"
  )

  predictions <- predict(ridge_model, test_matrix, type = "class")
  embed_accuracy <- mean(predictions == test_data$label)
} else {
  embed_accuracy <- NA
}

embed_time <- as.numeric(difftime(time_embed_end, time_embed_start, units = "secs"))

# Approach 2: Fine-tuning (if sufficient data available)
# Note: Requires more data for realistic results
# This is illustrative - production use requires larger datasets

benchmark_results <- tibble(
  Approach = c(
    "Fixed Embeddings + Ridge",
    "Fine-Tuning (requires larger dataset)"
  ),
  `Training Time (sec)` = c(
    round(embed_time, 1),
    "N/A - needs 1000+ examples"
  ),
  `Accuracy` = c(
    if (!is.na(embed_accuracy)) round(embed_accuracy, 2) else "N/A",
    "N/A - needs larger dataset"
  ),
  Hardware = c("CPU", "GPU recommended")
)

knitr::kable(benchmark_results, caption = "Performance comparison on sample task")
```

**Note**: This benchmark uses a minimal dataset for demonstration. Production benchmarks require 1000+ training examples for meaningful comparison. Fine-tuning typically shows 5-10% accuracy improvements on domain-specific tasks with sufficient data, but requires GPU acceleration and longer training times.

### Domain Adaptation

Fine-tuning shows substantial gains when the domain differs from the pre-training corpus. The performance improvement scales with:

- Domain specificity (medical, legal, technical text shows larger gains)
- Dataset size (1000+ examples needed for stable fine-tuning)
- Quality of labels (clean, consistent annotations improve results)

For specialized domains, fine-tuning can improve F1 scores by 10-20 percentage points compared to fixed embeddings, but requires careful validation to avoid overfitting.

## Practical Recommendations

### Decision Framework

1. **Start with fixed embeddings** for rapid prototyping
2. **Evaluate baseline performance** on validation set
3. **Consider fine-tuning if**:
   - Baseline accuracy insufficient for application requirements
   - Domain is specialized (medical, legal, scientific)
   - Sufficient training data available (1000+ examples)
   - GPU resources accessible

### Hybrid Approach

For optimal results with limited resources:

1. Extract embeddings on full dataset (CPU)
2. Use embeddings for initial model selection and hyperparameter tuning
3. Fine-tune final model on subset of most informative examples (GPU)
4. Validate on held-out test set

### Cost-Benefit Analysis

**Research and Development**
- Use fixed embeddings for exploration and hypothesis testing
- Reserve fine-tuning for final model optimization

**Production Systems**
- Fine-tune for maximum accuracy on critical applications
- Cache embeddings for high-throughput inference scenarios

## Limitations and Considerations

### Fixed Embeddings

- Cannot adapt to domain-specific terminology
- May miss subtle contextual patterns
- Performance ceiling limited by pre-training

### Fine-Tuning

- Risk of overfitting with small datasets
- Requires careful hyperparameter tuning
- Computationally expensive
- Longer iteration cycles during development

## Advanced Topics

### Contrastive Learning

Improve embeddings through contrastive pre-training on unlabeled domain data before classification:

```r
# Conceptual workflow:
# 1. Generate positive pairs (paraphrases, augmentations)
# 2. Fine-tune with contrastive loss
# 3. Add classification head and fine-tune again
```

### Few-Shot Learning

Leverage pre-trained embeddings for few-shot classification:

```r
# Prototypical networks approach:
# 1. Generate embeddings for support set (few examples per class)
# 2. Compute class prototypes (mean embeddings)
# 3. Classify query examples by nearest prototype
```

## Conclusion

The choice between fixed embeddings and fine-tuning depends on task requirements, data availability, and computational resources. Fixed embeddings provide efficient, interpretable baselines, while fine-tuning enables state-of-the-art performance on specialized tasks. For most applications, starting with fixed embeddings and selectively applying fine-tuning to high-value use cases represents an optimal strategy.

## References

- Granite Embedding R2 Models (2025): https://arxiv.org/html/2508.21085v1 - The research paper introducing the model architecture, training methodology, and benchmarks
- Granite Models on Hugging Face: https://huggingface.co/ibm-granite
- Fine-Tuning Best Practices: https://huggingface.co/docs/transformers/training
- Transfer Learning in NLP: Howard and Ruder (2018), Universal Language Model Fine-tuning for Text Classification
