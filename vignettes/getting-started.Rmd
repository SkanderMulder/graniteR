---
title: "Getting Started with graniteR"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Getting Started with graniteR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  eval = FALSE
)
```

## Introduction

The **graniteR** package provides a pipe-friendly interface to IBM's Granite embedding models. It follows tidyverse conventions and makes it easy to generate embeddings and train text classifiers.

## Installation

### R Package Installation

```{r}
# Install from GitHub
devtools::install_github("skandermulder/graniteR")
```

### Python Dependencies Setup

graniteR requires Python dependencies to function. The traditional `pip` installation can be slow (10-20 minutes for PyTorch and transformers). We recommend using **UV**, which is 10-100x faster.

#### Recommended: Fast Setup with UV

From your terminal, run the automated setup script:

```bash
cd path/to/graniteR
./setup_python.sh
```

This script will:
- Automatically install UV if not present
- Create a Python virtual environment at `.venv`
- Install all dependencies (transformers, torch, datasets, numpy) in 1-2 minutes

Then configure R to use this virtual environment by adding to your `.Rprofile` or script:

```{r}
Sys.setenv(RETICULATE_PYTHON = ".venv/bin/python")
```

#### Alternative: Traditional Installation

If you prefer the standard approach (slower):

```{r}
library(graniteR)
install_granite()  # Uses pip - may take 10-20 minutes
```

Or use UV from within R:

```{r}
library(graniteR)
install_granite_uv()  # Uses UV - much faster
```

### Verifying Setup

Test your installation:

```{r}
library(graniteR)
library(tibble)

# This should work without errors
tibble(text = "Test") |> granite_embed(text)
```

## Generating Embeddings

The simplest use case is generating sentence embeddings:

```{r}
library(graniteR)
library(dplyr)
library(tibble)

data <- tibble(
  id = 1:3,
  text = c(
    "The quick brown fox jumps over the lazy dog",
    "Machine learning is a subset of artificial intelligence",
    "R is a language for statistical computing"
  )
)

embeddings <- data |>
  granite_embed(text)

dim(embeddings)
```

The `granite_embed()` function adds embedding columns (emb_1, emb_2, ..., emb_768) to your data.

## Text Classification

### Basic Classification

```{r}
train_data <- tibble(
  text = c(
    "I love this product, it's amazing!",
    "This is terrible, worst purchase ever",
    "Great quality and fast shipping",
    "Very disappointed with this item",
    "Excellent service, highly recommend",
    "Poor quality, don't waste your money"
  ),
  sentiment = c(1, 0, 1, 0, 1, 0)
)

classifier <- granite_classifier(num_labels = 2) |>
  granite_train(
    train_data,
    text,
    sentiment,
    epochs = 5,
    batch_size = 2,
    learning_rate = 2e-5,
    validation_split = 0.3
  )
```

### Making Predictions

```{r}
new_data <- tibble(
  text = c(
    "This product exceeded my expectations",
    "Not worth the price"
  )
)

# Get class predictions
predictions <- granite_predict(classifier, new_data, text, type = "class")
print(predictions)

# Get probability scores
probabilities <- granite_predict(classifier, new_data, text, type = "prob")
print(probabilities)
```

## Advanced Usage

### Using GPU

If you have a CUDA-capable GPU:

```{r}
model <- granite_model(task = "embedding", device = "cuda")
classifier <- granite_classifier(num_labels = 3, device = "cuda")
```

### Custom Models

You can use any compatible model from Hugging Face:

```{r}
custom_model <- granite_model(
  model_name = "sentence-transformers/all-MiniLM-L6-v2",
  task = "embedding"
)

embeddings <- data |>
  granite_embed(text, model = custom_model)
```

### Working with Factors

The package automatically handles factor labels:

```{r}
train_data <- tibble(
  text = c("positive text", "negative text", "neutral text"),
  category = factor(c("pos", "neg", "neu"))
)

classifier <- granite_classifier(num_labels = 3) |>
  granite_train(train_data, text, category, epochs = 3)
```

## Model Information

The default model is IBM's Granite Embedding English R2:

- **Model**: `ibm-granite/granite-embedding-english-r2`
- **Parameters**: 149M
- **Embedding Dimension**: 768
- **Max Sequence Length**: 512 tokens (model supports up to 8,192 in research)
- **Use Case**: Sentence embeddings, retrieval, classification

This model is based on the [Granite Embedding R2 Models](https://arxiv.org/html/2508.21085v1) research, featuring ModernBERT architecture with Flash Attention optimizations and achieving 19-44% speed advantages over leading competitors while maintaining superior accuracy.

## Tips

1. **Batch Size**: Adjust based on your GPU memory. Smaller batches use less memory.
2. **Learning Rate**: Start with 2e-5 or 5e-5 for fine-tuning.
3. **Epochs**: 3-5 epochs is typically sufficient for small datasets.
4. **Validation**: Always use a validation split to monitor overfitting.

## Next Steps

- Try different Granite or transformer models
- Experiment with multi-class classification
- Use embeddings for clustering or similarity search
- Fine-tune on domain-specific data
