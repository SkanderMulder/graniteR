% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/moe_classifier.R
\name{moe_classifier}
\alias{moe_classifier}
\title{Create a Mixture of Experts (MoE) text classifier}
\usage{
moe_classifier(
  num_labels = NULL,
  num_experts = NULL,
  data = NULL,
  label_col = NULL,
  model_name = "ibm-granite/granite-embedding-english-r2",
  device = NULL,
  freeze_backbone = FALSE,
  hidden_dim = NULL,
  dropout = 0.2,
  expert_depth = 2
)
}
\arguments{
\item{num_labels}{Number of output classes}

\item{num_experts}{Number of expert networks (default: 4 for multi-class, 3 for binary)}

\item{data}{Optional training data frame to infer num_labels from}

\item{label_col}{Optional label column name (unquoted) to infer num_labels from}

\item{model_name}{Model identifier from Hugging Face Hub}

\item{device}{Device to use ("cpu" or "cuda"). If NULL, automatically detects GPU availability.}

\item{freeze_backbone}{Whether to freeze the pretrained backbone (default: FALSE for MoE)}

\item{hidden_dim}{Hidden dimension for expert networks (default: backbone_size)}

\item{dropout}{Dropout probability for expert networks (default: 0.2)}

\item{expert_depth}{Number of layers per expert network (default: 2)}
}
\value{
A MoE classifier object with model and tokenizer
}
\description{
Creates a text classifier using a Mixture of Experts architecture. This advanced
approach uses multiple specialized expert networks whose outputs are dynamically
weighted by a gating network, potentially improving performance on multi-class tasks.
}
\details{
The Mixture of Experts (MoE) architecture provides several advantages:
\itemize{
  \item Multiple specialized experts learn different aspects of the task
  \item Dynamic gating network weights experts per input
  \item Can improve accuracy on complex multi-class problems
  \item Load balancing encourages diverse expert usage
}

The architecture consists of:
\itemize{
  \item Pretrained backbone (usually unfrozen for MoE to be effective)
  \item N expert networks (deeper feed-forward heads)
  \item Gating network that learns to weight experts
  \item Load balancing loss to encourage expert diversity
}

**Important**: MoE works best with freeze_backbone=FALSE. With frozen backbone,
the standard classifier often performs similarly or better due to simpler optimization.
MoE requires more training data and compute but can achieve higher accuracy on
complex multi-class tasks when properly tuned.
}
\examples{
\dontshow{if (requireNamespace("transformers")) withAutoprint(\{ # examplesIf}
# Create MoE classifier for emotion detection (6 classes)
clf <- moe_classifier(num_labels = 6, num_experts = 4)

# Binary classification with 3 experts
clf_binary <- moe_classifier(num_labels = 2, num_experts = 3)

# Infer from data
data <- tibble::tibble(text = c("a", "b", "c"), label = c("joy", "sad", "angry"))
clf <- moe_classifier(data = data, label_col = label)
\dontshow{\}) # examplesIf}
}
\seealso{
\code{\link{train_moe}}, \code{\link{predict.moe_classifier}}
}
