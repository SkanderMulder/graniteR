% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/classifier.R
\name{classifier}
\alias{classifier}
\title{Create a text classifier}
\usage{
classifier(
  num_labels = NULL,
  data = NULL,
  label_col = NULL,
  model_name = "ibm-granite/granite-embedding-english-r2",
  device = NULL,
  freeze_backbone = TRUE
)
}
\arguments{
\item{num_labels}{Number of output classes (e.g., 2 for binary, 4 for multi-class).
If NULL and data is provided, will be inferred from unique labels.}

\item{data}{Optional training data frame to infer num_labels from}

\item{label_col}{Optional label column name (unquoted) to infer num_labels from}

\item{model_name}{Model identifier from Hugging Face Hub}

\item{device}{Device to use ("cpu" or "cuda"). If NULL, automatically detects GPU availability.}

\item{freeze_backbone}{Whether to freeze the pretrained backbone (default: TRUE).
Set to FALSE for full fine-tuning which may improve accuracy but requires more
compute, time, and careful learning rate tuning.}
}
\value{
A classifier object with model and tokenizer
}
\description{
Creates a text classifier using transformer encoder models. Supports both
binary classification (2 classes) and multi-class classification (3+ classes).
The number of classes can be specified explicitly or inferred from training data.
}
\details{
The pretrained base model is frozen and only the classification head is trained.
This approach is more efficient and prevents overfitting on smaller datasets
while leveraging the pretrained representations.


By default, the classifier uses a frozen pretrained model with only the classification
head being trainable. This provides several advantages:
\itemize{
  \item Faster training (fewer parameters to update)
  \item Lower memory requirements
  \item Better generalization on small datasets
  \item Preserves pretrained knowledge
  \item Works well with higher learning rates (1e-3)
}

For full fine-tuning (freeze_backbone=FALSE):
\itemize{
  \item Use lower learning rates (2e-5 to 5e-5)
  \item Expect longer training time
  \item May achieve slightly higher accuracy with enough data
  \item Requires more GPU memory
}
}
\examples{
\dontshow{if (requireNamespace("transformers")) withAutoprint(\{ # examplesIf}
# Explicit num_labels
clf <- classifier(num_labels = 2)

# Infer from data
data <- tibble::tibble(text = c("a", "b"), label = c("high", "low"))
clf <- classifier(data = data, label_col = label)
\dontshow{\}) # examplesIf}
}
\seealso{
\code{\link{train}}, \code{\link{predict}}
}
